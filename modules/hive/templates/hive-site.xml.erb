<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
<!-- that are implied by Hadoop setup variables.                                                -->
<!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
<!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
<!-- resource).                                                                                 -->

<!-- Hive Execution Parameters -->
<property>
  <name>hive.exec.scratchdir</name>
  <value>/tmp/hive-${user.name}</value>
  <description>Scratch space for Hive jobs</description>
</property>

<property>
  <name>hive.metastore.local</name>
  <value>true</value>
  <description>controls whether to connect to remove metastore server or open a new metastore server in Hive Client JVM</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://<%=mysqlserver%>/hive?createDatabaseIfNotExist=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>


<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hiveuser</value>
  <description>Driver class name for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hivepassword</value>
  <description>Driver class name for a JDBC metastore</description>
</property>


<property>
  <name>hive.metastore.metadb.dir</name>
  <value>file:///var/metastore/metadb/</value>
  <description>The location of filestore metadata base dir</description>
</property>

<property>
  <name>hive.metastore.uris</name>
  <value>file:///var/lib/hivevar/metastore/metadb/</value>
  <description>Comma separated list of URIs of metastore servers. The first server that can be connected to will be used.</description>
</property>

<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>hdfs://<%=namenode_name%>:54310/user/hive/warehouse</value>
  <description>location of default database for the warehouse</description>
</property>

<property>
  <name>hive.metastore.connect.retries</name>
  <value>5</value>
  <description>Number of retries while opening a connection to metastore</description>
</property>

<property>
  <name>hive.metastore.rawstore.impl</name>
  <value>org.apache.hadoop.hive.metastore.ObjectStore</value>
  <description>Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database</description>
</property>

<property>
  <name>hive.default.fileformat</name>
  <value>TextFile</value>
  <description>Default file format for CREATE TABLE statement. Options are TextFile and SequenceFile. Users can explicitly say CREATE TABLE ... STORED AS &lt;TEXTFILE|SEQUENCEFILE&gt; to override</description>
</property>

<property>
  <name>hive.map.aggr</name>
  <value>false</value>
  <description>Whether to use map-side aggregation in Hive Group By queries</description>
</property>

<property>
  <name>hive.join.emit.interval</name>
  <value>1000</value>
  <description>How many rows in the right-most join operand Hive should buffer before emitting the join result. </description>
</property>

<property>
  <name>hive.exec.script.maxerrsize</name>
  <value>100000</value>
  <description>Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). This prevents runaway scripts from filling logs partitions to capacity </description>
</property>

<property>
  <name>hive.exec.compress.output</name>
  <value>false</value>
  <description> This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
</property>

<property>
  <name>hive.exec.compress.intermediate</name>
  <value>false</value>
  <description> This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
</property>

<property>
  <name>hive.hwi.listen.host</name>
  <value>0.0.0.0</value>
  <description>This is the host address the Hive Web Interface will listen on</description>
</property>
 
<property>
  <name>hive.hwi.listen.port</name>
  <value>9999</value>
  <description>This is the port the Hive Web Interface will listen on</description>
</property>

<property>
  <name>hive.hwi.war.file</name>
  <value>/usr/lib/hive/lib/hive-hwi-0.7.0-cdh3u0.war</value>
  <description>This is the WAR file with the jsp content for Hive Web Interface</description>
</property>

<property>
  <name>mapred.job.tracker</name>
  <value><%=jobtracker_name%>:54311</value>
  <description>This is the WAR file with the jsp content for Hive Web Interface</description>
</property>
<property>
<name>fs.default.name</name>
<value>hdfs://<%=namenode_name%>:54310</value>
</property>

<property>
<name>hive.aux.jars.path</name>
<value>file:////usr/lib/hive/lib/hive-hbase-handler-0.7.0-cdh3u0.jar</value>
<description>Location of AUX file. CUrrently used to read handlers</description>
</property>

<property>
  <name>hive.security.authorization.enabled</name>
  <value>false</value>
  <description>enable or disable the hive client authorization</description>
</property>

<property>
  <name>hive.security.authorization.createtable.user.grants</name>
  <value>root,oded,michael,ophir:ALL</value>
  <description>the privileges automatically granted to some users whenever a table gets created. 
   An example like "userX,userY:select;userZ:create" will grant select privilege to userX and userY, 
   and grant create privilege to userZ whenever a new table created.</description>
</property>

<property>
  <name>datanucleus.valuegeneration.transactionIsolation</name>
  <value>repeatable-read</value>
</property>

<property>
  <name>datanucleus.transactionIsolation</name>
  <value>repeatable-read</value>
</property>

</configuration>

